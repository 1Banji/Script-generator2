{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"MEA.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "#raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  86670\n",
      "Total Vocab:  86\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  86570\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "86570/86570 [==============================] - 82s 952us/step - loss: 3.2450\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.24501, saving model to weights-improvement-01-3.2450.hdf5\n",
      "Epoch 2/20\n",
      "86570/86570 [==============================] - 88s 1ms/step - loss: 2.9393\n",
      "\n",
      "Epoch 00002: loss improved from 3.24501 to 2.93932, saving model to weights-improvement-02-2.9393.hdf5\n",
      "Epoch 3/20\n",
      "86570/86570 [==============================] - 96s 1ms/step - loss: 2.8045\n",
      "\n",
      "Epoch 00003: loss improved from 2.93932 to 2.80452, saving model to weights-improvement-03-2.8045.hdf5\n",
      "Epoch 4/20\n",
      "86570/86570 [==============================] - 94s 1ms/step - loss: 2.6924\n",
      "\n",
      "Epoch 00004: loss improved from 2.80452 to 2.69239, saving model to weights-improvement-04-2.6924.hdf5\n",
      "Epoch 5/20\n",
      "86570/86570 [==============================] - 94s 1ms/step - loss: 2.6079\n",
      "\n",
      "Epoch 00005: loss improved from 2.69239 to 2.60791, saving model to weights-improvement-05-2.6079.hdf5\n",
      "Epoch 6/20\n",
      "86570/86570 [==============================] - 88s 1ms/step - loss: 2.5480\n",
      "\n",
      "Epoch 00006: loss improved from 2.60791 to 2.54803, saving model to weights-improvement-06-2.5480.hdf5\n",
      "Epoch 7/20\n",
      "86570/86570 [==============================] - 86s 998us/step - loss: 2.5108\n",
      "\n",
      "Epoch 00007: loss improved from 2.54803 to 2.51085, saving model to weights-improvement-07-2.5108.hdf5\n",
      "Epoch 8/20\n",
      "86570/86570 [==============================] - 89s 1ms/step - loss: 2.4680\n",
      "\n",
      "Epoch 00008: loss improved from 2.51085 to 2.46797, saving model to weights-improvement-08-2.4680.hdf5\n",
      "Epoch 9/20\n",
      "86570/86570 [==============================] - 84s 969us/step - loss: 2.4339\n",
      "\n",
      "Epoch 00009: loss improved from 2.46797 to 2.43387, saving model to weights-improvement-09-2.4339.hdf5\n",
      "Epoch 10/20\n",
      "86570/86570 [==============================] - 88s 1ms/step - loss: 2.3974\n",
      "\n",
      "Epoch 00010: loss improved from 2.43387 to 2.39738, saving model to weights-improvement-10-2.3974.hdf5\n",
      "Epoch 11/20\n",
      "86570/86570 [==============================] - 89s 1ms/step - loss: 2.3648\n",
      "\n",
      "Epoch 00011: loss improved from 2.39738 to 2.36480, saving model to weights-improvement-11-2.3648.hdf5\n",
      "Epoch 12/20\n",
      "86570/86570 [==============================] - 84s 973us/step - loss: 2.3322\n",
      "\n",
      "Epoch 00012: loss improved from 2.36480 to 2.33220, saving model to weights-improvement-12-2.3322.hdf5\n",
      "Epoch 13/20\n",
      "86570/86570 [==============================] - 90s 1ms/step - loss: 2.2975\n",
      "\n",
      "Epoch 00013: loss improved from 2.33220 to 2.29755, saving model to weights-improvement-13-2.2975.hdf5\n",
      "Epoch 14/20\n",
      "86570/86570 [==============================] - 88s 1ms/step - loss: 2.2753\n",
      "\n",
      "Epoch 00014: loss improved from 2.29755 to 2.27534, saving model to weights-improvement-14-2.2753.hdf5\n",
      "Epoch 15/20\n",
      "86570/86570 [==============================] - 86s 988us/step - loss: 2.2505\n",
      "\n",
      "Epoch 00015: loss improved from 2.27534 to 2.25051, saving model to weights-improvement-15-2.2505.hdf5\n",
      "Epoch 16/20\n",
      "86570/86570 [==============================] - 89s 1ms/step - loss: 2.2237\n",
      "\n",
      "Epoch 00016: loss improved from 2.25051 to 2.22373, saving model to weights-improvement-16-2.2237.hdf5\n",
      "Epoch 17/20\n",
      "86570/86570 [==============================] - 90s 1ms/step - loss: 2.2290\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.22373\n",
      "Epoch 18/20\n",
      "86570/86570 [==============================] - 102s 1ms/step - loss: 2.1845\n",
      "\n",
      "Epoch 00018: loss improved from 2.22373 to 2.18455, saving model to weights-improvement-18-2.1845.hdf5\n",
      "Epoch 19/20\n",
      "86570/86570 [==============================] - 99s 1ms/step - loss: 2.1579\n",
      "\n",
      "Epoch 00019: loss improved from 2.18455 to 2.15788, saving model to weights-improvement-19-2.1579.hdf5\n",
      "Epoch 20/20\n",
      "86570/86570 [==============================] - 100s 1ms/step - loss: 2.1386\n",
      "\n",
      "Epoch 00020: loss improved from 2.15788 to 2.13865, saving model to weights-improvement-20-2.1386.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x291df125048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.1386.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" NS:\n",
      "\n",
      "-Milky Way departure\n",
      "\n",
      "-Asari archaeologist: Dr. Liara Tâ€™Soni\n",
      "\n",
      "-Final entry\n",
      "\n",
      " \n",
      "\n",
      "ALEC RYDER:\n",
      "\n",
      "Dun \"\n",
      " yhe keat oo the toat oo the toation  aut the toat oo the toation  aut the toat oo the cettone the cete to the toation  aut the seat oo the toation  aut the seat oo the toation  ant the toat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the seat oo the toation  aut the se\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
