{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"MEA.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  86670\n",
      "Total Vocab:  86\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  86570\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "86570/86570 [==============================] - 361s 4ms/step - loss: 3.1132\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.11316, saving model to weights-improvement-01-3.1132-bigger.hdf5\n",
      "Epoch 2/50\n",
      "86570/86570 [==============================] - 351s 4ms/step - loss: 2.7274\n",
      "\n",
      "Epoch 00002: loss improved from 3.11316 to 2.72741, saving model to weights-improvement-02-2.7274-bigger.hdf5\n",
      "Epoch 3/50\n",
      "86570/86570 [==============================] - 352s 4ms/step - loss: 2.5716\n",
      "\n",
      "Epoch 00003: loss improved from 2.72741 to 2.57156, saving model to weights-improvement-03-2.5716-bigger.hdf5\n",
      "Epoch 4/50\n",
      "86570/86570 [==============================] - 329s 4ms/step - loss: 2.4551\n",
      "\n",
      "Epoch 00004: loss improved from 2.57156 to 2.45515, saving model to weights-improvement-04-2.4551-bigger.hdf5\n",
      "Epoch 5/50\n",
      "86570/86570 [==============================] - 321s 4ms/step - loss: 2.3757\n",
      "\n",
      "Epoch 00005: loss improved from 2.45515 to 2.37569, saving model to weights-improvement-05-2.3757-bigger.hdf5\n",
      "Epoch 6/50\n",
      "86570/86570 [==============================] - 336s 4ms/step - loss: 2.3180\n",
      "\n",
      "Epoch 00006: loss improved from 2.37569 to 2.31799, saving model to weights-improvement-06-2.3180-bigger.hdf5\n",
      "Epoch 7/50\n",
      "86570/86570 [==============================] - 350s 4ms/step - loss: 2.2650\n",
      "\n",
      "Epoch 00007: loss improved from 2.31799 to 2.26505, saving model to weights-improvement-07-2.2650-bigger.hdf5\n",
      "Epoch 8/50\n",
      "86570/86570 [==============================] - 335s 4ms/step - loss: 2.2131\n",
      "\n",
      "Epoch 00008: loss improved from 2.26505 to 2.21310, saving model to weights-improvement-08-2.2131-bigger.hdf5\n",
      "Epoch 9/50\n",
      "86570/86570 [==============================] - 326s 4ms/step - loss: 2.1676\n",
      "\n",
      "Epoch 00009: loss improved from 2.21310 to 2.16756, saving model to weights-improvement-09-2.1676-bigger.hdf5\n",
      "Epoch 10/50\n",
      "86570/86570 [==============================] - 312s 4ms/step - loss: 2.1218\n",
      "\n",
      "Epoch 00010: loss improved from 2.16756 to 2.12181, saving model to weights-improvement-10-2.1218-bigger.hdf5\n",
      "Epoch 11/50\n",
      "86570/86570 [==============================] - 326s 4ms/step - loss: 2.0786\n",
      "\n",
      "Epoch 00011: loss improved from 2.12181 to 2.07861, saving model to weights-improvement-11-2.0786-bigger.hdf5\n",
      "Epoch 12/50\n",
      "86570/86570 [==============================] - 311s 4ms/step - loss: 2.0411\n",
      "\n",
      "Epoch 00012: loss improved from 2.07861 to 2.04113, saving model to weights-improvement-12-2.0411-bigger.hdf5\n",
      "Epoch 13/50\n",
      "15744/86570 [====>.........................] - ETA: 4:27 - loss: 1.9875"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
